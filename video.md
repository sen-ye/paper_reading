[toc]

# CoDeF: Content Deformation Fields for Temporally Consistent Video Processing

**keywords:** content deformation field, video representation, temporal deformation field

**summary:**





# NeRV: Neural Representations for Videos

**keywords:** neural representation, video, NeRV

**summary:**

**main idea: represent video as a function of time**

NeRV architecture

* **input embedding**: maps $t$ to high dimensional space
* **loss function**: combination of L1 loss and SSIM loss

<img src="E:\github_repos\paper_reading\fig\NeRV.png" alt="NeRV" style="zoom:67%;" />

# E-NeRV: Expedite Neural Video Representation with Disentangled Spatial-Temporal Context

**keywords:** neural implicit representation, video, disentanglement

**summary:**

**core idea:** disentangle the spatial-temporal information

![e-nerv](E:\github_repos\paper_reading\fig\e-nerv.png)

# HNeRV: A Hybrid Neural Representation for Videos

**keywords:** HNeRV, implicit and explicit representation, content-adaptive embedding

**summary:**

![HNeRV](E:\github_repos\paper_reading\fig\HNeRV.png)

# NIRVANA: Neural Implicit Representations of Videos with Adaptive Networks and Autoregressive Patch-wise Modeling

**keywords:** implicit neural representation, video, temporal redundancy

**summary:**

![NIRVANA](E:\github_repos\paper_reading\fig\NIRVANA.png)

# FFNeRV: Flow-Guided Frame-Wise Neural Representations for Videos

**keywords:** NeRV, optical flow, feature gird, temporal redundancy

**summary:**

**core idea:** aggregate neighboring frames through optical flow

![FFNeRV](E:\github_repos\paper_reading\fig\FFNeRV.png)

# GENERATING VIDEOS WITH DYNAMICS-AWARE IMPLICIT GENERATIVE ADVERSARIAL NETWORKS

**keywords:** GAN, INR, video generation, dynamics-aware

**summary:**

**core idea:**

* INR-based video generator: decompose content and motion feature
* motion discriminator that efficiently detects unnatural motions

![DiGAN](E:\github_repos\paper_reading\fig\DiGAN.png)

# DECOMPOSING MOTION AND CONTENT FOR NATURAL VIDEO SEQUENCE PREDICTION

**keywords:** video generation, motion and content decomposition

**summary:**

Architecture

* **motion encoder** takes differences of two neighboring frames as input
* **content encoder** takes last frame as input



![mcnet](E:\github_repos\paper_reading\fig\mcnet.png)

# StableVideo: Text-driven Consistency-aware Diffusion Video Editing

**keywords:**

**summary:**





































